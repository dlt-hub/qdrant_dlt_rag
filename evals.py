import logging
from typing import Dict, Tuple, Optional

from deepeval.dataset import EvaluationDataset
from openai import OpenAI
from deepeval.metrics import  AnswerRelevancyMetric

from deepeval.evaluator import run_test, assert_test
from deepeval.test_case import LLMTestCase
from deepeval.evaluator import TestResult
from dotenv import load_dotenv
load_dotenv()
import os
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

from qdrant_client import QdrantClient
QDRANT_CLIENT = os.getenv('QDRANT_CLIENT')
QDRANT_KEY = os.getenv('QDRANT_KEY')



def qdrant_client(collection_name:str="detailed_info_content", original_query:str="Who is an avid reader and writer?"):

    qdrant_client = QdrantClient(QDRANT_CLIENT,
                                 api_key=QDRANT_KEY,
                                 )

    result = qdrant_client.query(
        collection_name=collection_name,
        query_text=original_query
    )
    return result



def generate_chatgpt_output(query: str, context: str = None, api_key=None, model_name="gpt-3.5-turbo"):
    """
    Generate a response from the OpenAI ChatGPT model.

    Args:
        query (str): The user's query or message.
        context (str, optional): Additional context for the conversation. Defaults to an empty string.
        api_key (str, optional): Your OpenAI API key. If not provided, the globally configured API key will be used.
        model_name (str, optional): The name of the ChatGPT model to use. Defaults to "gpt-3.5-turbo".

    Returns:
        str: The response generated by the ChatGPT model.

    Raises:
        Exception: If an error occurs during the API call, an error message is returned for the caller to handle.
    """
    if not context:
        context = ""

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "assistant", "content": context},
        {"role": "user", "content": query},
    ]

    try:
          # Use the provided API key or the one set globally

        response = client.chat.completions.create(model=model_name,
        messages=messages)

        llm_output = response.choices[0].message.content
        return llm_output
    except Exception as e:
        return f"An error occurred: {e}"  # Return the error message for the caller to handle



def eval_test(
    query=None,
    output=None,
    expected_output=None,
    context=None,
    qdrant_collection_name=None,
):
    logging.info("Generating chatgpt output")
    if context is None:
        context = qdrant_client(collection_name=qdrant_collection_name, original_query=query)
    result_output = generate_chatgpt_output(query, str(context))

    logging.info("Moving on with chatgpt output")
    test_case = LLMTestCase(
        input=str(query),
        actual_output=str(result_output),
        expected_output=str(expected_output),
        retrieval_context=[str(context)],
    )
    metric = AnswerRelevancyMetric()
    metric.measure(test_case)
    return metric.score

qa_pairs_example = {
    "What is the capital of France?": ("Paris", None),
    "Who wrote 'To Kill a Mockingbird'?": ("Harper Lee", None),
    "What is the chemical formula for water?": ("H2O", None),
    "Who painted the Mona Lisa?": ("Leonardo da Vinci", None),
    "What is the speed of light?": ("299,792,458 meters per second", None),
    "What is the largest mammal?": ("Blue Whale", None),
    "Who discovered penicillin?": ("Alexander Fleming", None),
    "What year did World War II end?": ("1945", None),
    "Who is the CEO of Tesla?": ("Elon Musk", None),
    # Note: This information is based on the status as of April 2023.
    "What is the currency of Japan?": ("Yen", None)
}

collections = [
    {"naive_llm": [{"dataset": 1}, {"dataset": 2}, {"dataset": 3}]},
    {"unstructured": [{"dataset": 1}, {"dataset": 2}, {"dataset": 3}]},
    {"structured": [{"dataset": 1}, {"dataset": 2}, {"dataset": 3}]}
]

def process_collection(collection_type, dataset):
    if collection_type == "naive_llm":
        file_number = dataset["dataset"]
        file_name = f"synthetic_data/naive_llm_element{file_number}.txt"
        file_path = os.path.join("synthetic_data", file_name)

        # Load the file here
        with open(file_path, 'r') as file:
            file_content = file.read()
            return file_content
    elif collection_type == "unstructured":
        pass
    elif collection_type == "structured":
        pass


def evaluate_qa_pairs(
    qa_pairs: Dict[str, Tuple[str, Optional[str]]],
    collections: list):

    results = []
    for collection in collections:
        for collection_type, datasets in collection.items():
            for dataset in datasets:
                context = process_collection(collection_type, dataset)
                for query, (expected_output, retrieval_context) in qa_pairs.items():
                    test_result = eval_test(query, expected_output, context, collection_type)
                    results.append(test_result)
    return results

evaluate_qa_pairs(qa_pairs_example, collections)

    # If you want to run the test
    # test_result = run_test(test_case, metrics=[metric])
    # print(test_result)
    # print(test_result)

    # dataset = EvaluationDataset(test_cases=[test_case,test_case_two])
    #
    #
    # ff = dataset.evaluate([metric])
    # print(ff)

    # def test_result_to_dict(test_result):
    #     return {
    #         "success": test_result.success,
    #         "score": test_result.score,
    #         "metric_name": test_result.metric_name,
    #         "query": test_result.query,
    #         "output": test_result.output,
    #         "expected_output": test_result.expected_output,
    #         "metadata": test_result.metadata,
    #         "context": test_result.context,
    #     }
    #
    # rr = metric.measure(LLMTestCase)
    # print(rr.)
    # ss = assert_test(test_case, metrics=[metric])
    # print(ss)
    # test_result_dict = []
    # for test in test_result:
    #     print(test)
    #     # test_result_it = test_result_to_dict(test)
    #     # test_result_dict.append(test_result_it)
    # return test_result_dict



