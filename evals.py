import logging
from typing import Dict, Tuple, Optional

from deepeval.dataset import EvaluationDataset
from openai import OpenAI
from deepeval.metrics import  AnswerRelevancyMetric

from deepeval.evaluator import run_test, assert_test
from deepeval.test_case import LLMTestCase
from deepeval.evaluator import TestResult
from dotenv import load_dotenv
load_dotenv()
import os
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

from qdrant_client import QdrantClient
QDRANT_CLIENT = os.getenv('QDRANT_CLIENT')
QDRANT_KEY = os.getenv('QDRANT_KEY')



def qdrant_client(collection_name:str="detailed_info_content", original_query:str="Who is an avid reader and writer?"):

    qdrant_client = QdrantClient(QDRANT_CLIENT,
                                 api_key=QDRANT_KEY,
                                 )

    result = qdrant_client.query(
        collection_name=collection_name,
        query_text=original_query
    )
    return result



def generate_chatgpt_output(query: str, context: str = None, api_key=None, model_name="gpt-3.5-turbo"):
    """
    Generate a response from the OpenAI ChatGPT model.

    Args:
        query (str): The user's query or message.
        context (str, optional): Additional context for the conversation. Defaults to an empty string.
        api_key (str, optional): Your OpenAI API key. If not provided, the globally configured API key will be used.
        model_name (str, optional): The name of the ChatGPT model to use. Defaults to "gpt-3.5-turbo".

    Returns:
        str: The response generated by the ChatGPT model.

    Raises:
        Exception: If an error occurs during the API call, an error message is returned for the caller to handle.
    """
    if not context:
        context = ""

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "assistant", "content": context},
        {"role": "user", "content": query},
    ]

    try:
          # Use the provided API key or the one set globally

        response = client.chat.completions.create(model=model_name,
        messages=messages)

        llm_output = response.choices[0].message.content
        return llm_output
    except Exception as e:
        return f"An error occurred: {e}"  # Return the error message for the caller to handle



def eval_test(
    query=None,
    output=None,
    expected_output=None,
    context=None,
    qdrant_collection_name=None,
):
    logging.info("Generating chatgpt output")
    if context is None:
        try:
            context = qdrant_client(collection_name=qdrant_collection_name, original_query=query)
        except:
            pass

    result_output = generate_chatgpt_output(query, str(context))

    logging.info("Moving on with chatgpt output")
    test_case = LLMTestCase(
        input=str(query),
        actual_output=str(result_output),
        expected_output=str(expected_output),
        retrieval_context=[str(context)],
    )
    metric = AnswerRelevancyMetric()
    metric.measure(test_case)
    print(metric.score)
    return metric.score

qa_pairs_example = {
    "What is the capital of France?": ("Paris", None),
    "Who wrote 'To Kill a Mockingbird'?": ("Harper Lee", None),
    # "What is the chemical formula for water?": ("H2O", None),
    # "Who painted the Mona Lisa?": ("Leonardo da Vinci", None),
    # "What is the speed of light?": ("299,792,458 meters per second", None),
    # "What is the largest mammal?": ("Blue Whale", None),
    # "Who discovered penicillin?": ("Alexander Fleming", None),
    # "What year did World War II end?": ("1945", None),
    # "Who is the CEO of Tesla?": ("Elon Musk", None),
    # # Note: This information is based on the status as of April 2023.
    # "What is the currency of Japan?": ("Yen", None)
}


import os

def process_collection(dataset, collection_type=None):
    """
    Process a collection based on the specified collection type. If the collection type is not provided,
    it defaults to iterating through known types.

    Parameters:
    - dataset: The dataset information.
    - collection_type (optional): The type of collection to process. If not provided, a default iteration is used.

    Returns:
    Tuple of file name and file content.
    """
    if collection_type:
        file_number = dataset["dataset"]
        file_name = f"{collection_type}_dataset_{file_number}.txt"
        file_path = os.path.join("synthetic_data", file_name)
        with open(file_path, 'r') as file:
            file_content = file.read()
            return file_name, file_content

    # Default iteration when collection_type is not provided
    for ctype in ["naive_llm", "unstructured", "structured"]:
        file_number = dataset["dataset"]
        file_name = f"{ctype}_dataset_{file_number}.txt"
        file_path = os.path.join("synthetic_data", file_name)
        try:
            with open(file_path, 'r') as file:
                file_content = file.read()
                return file_name, file_content
        except FileNotFoundError:
            continue

    raise ValueError("Unable to process the dataset with given collection types")



def evaluate_qa_pairs(
    qa_pairs: Dict[str, Tuple[str, Optional[str]]],
    collections: list,
    specific_collection_type: str = None):
    """
    Evaluates QA pairs against provided collections. Optionally, evaluates against a specific collection type.

    Parameters:
    - qa_pairs (Dict): A dictionary of question-answer pairs.
    - collections (list): A list of collections to evaluate against.
    - specific_collection_type (str, optional): Specific collection type to evaluate against.
      If None, evaluates against all types in the collections list.

    Returns:
    - List of dictionaries containing evaluation results.
    """
    results = []
    for collection in collections:
        for collection_type, datasets in collection.items():
            if specific_collection_type and collection_type != specific_collection_type:
                continue

            for dataset in datasets:
                file_name, context = process_collection(dataset, collection_type)
                for query, (expected_output, retrieval_context) in qa_pairs.items():
                    score = eval_test(query, expected_output, context, collection_type)
                    result_details = {
                        "question": query,
                        "expected_answer": expected_output,
                        "score": score,
                        "file_name": file_name,
                        "eval_type": collection_type
                    }
                    results.append(result_details)
    return results



collections = [
    {"naive_llm": [{"dataset": 1}, {"dataset": 2}, {"dataset": 3}]},
    {"unstructured": [{"dataset": 1}, {"dataset": 2}, {"dataset": 3}]},
    # {"structured": [{"dataset": 1}, {"dataset": 2}, {"dataset": 3}]}
]

# eval_test(query="What is the capital of France?", expected_output="Paris", qdrant_collection_name="naive_llm_dataset_1.txt")

evaluate_qa_pairs(qa_pairs_example, collections)

