import logging
from typing import Dict, Tuple, Optional

from deepeval.dataset import EvaluationDataset
from openai import OpenAI
from ragas_custom import  RagasMetric


from deepeval.evaluate import run_test, assert_test
from deepeval.test_case import LLMTestCase
from deepeval.evaluate import TestResult
from dotenv import load_dotenv
load_dotenv()
import os
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

from qdrant_client import QdrantClient
QDRANT_CLIENT = os.getenv('QDRANT_CLIENT')
QDRANT_KEY = os.getenv('QDRANT_KEY')

from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores.qdrant import Qdrant

from deepeval.metrics import ContextualPrecisionMetric
from deepeval.metrics import ContextualRelevancyMetric
from deepeval.metrics import ContextRecallMetric


os.environ["TOKENIZERS_PARALLELISM"] = "false"

def qdrant_client(collection_name:str="detailed_info_content", original_query:str="Who is an avid reader and writer?"):

    qdrant_client = QdrantClient(QDRANT_CLIENT,
                                 api_key=QDRANT_KEY,
                                 )

    result = qdrant_client.query(
        collection_name=collection_name,
        query_text=original_query
    )


    return result

def generate_chatgpt_output(query: str, context: str = None, api_key=None, model_name="gpt-3.5-turbo"):
    """
    Generate a response from the OpenAI ChatGPT model.

    Args:
        query (str): The user's query or message.
        context (str, optional): Additional context for the conversation. Defaults to an empty string.
        api_key (str, optional): Your OpenAI API key. If not provided, the globally configured API key will be used.
        model_name (str, optional): The name of the ChatGPT model to use. Defaults to "gpt-3.5-turbo".

    Returns:
        str: The response generated by the ChatGPT model.

    Raises:
        Exception: If an error occurs during the API call, an error message is returned for the caller to handle.
    """
    if not context:
        context = ""

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "assistant", "content": context},
        {"role": "user", "content": query},
    ]

    try:
          # Use the provided API key or the one set globally

        response = client.chat.completions.create(model=model_name,
        messages=messages)

        llm_output = response.choices[0].message.content
        return llm_output
    except Exception as e:
        return f"An error occurred: {e}"  # Return the error message for the caller to handle



def eval_test(
    query=None,
    expected_output=None,
    context=None,
    qdrant_collection_name=None,
):
    logging.info("Generating chatgpt output")
    if context is None:
        try:
            try:
                context = qdrant_client(collection_name=qdrant_collection_name, original_query=query)
            except:
                embeddings = OpenAIEmbeddings(model="text-embedding-ada-002", openai_api_key=OPENAI_API_KEY)
                qdrant_client_url = os.getenv('QDRANT_CLIENT')
                qdrant_api_key = os.getenv('QDRANT_KEY')

                if not qdrant_client_url or not qdrant_api_key:
                    logging.error("QDRANT_CLIENT or QDRANT_KEY environment variables not set.")
                    return None
                client = QdrantClient(api_key=qdrant_api_key, url=qdrant_client_url)
                qdrant = Qdrant(client, qdrant_collection_name, embeddings)
                context = qdrant.search(query=query, search_type="similarity")
        except:
            pass

    result_output = generate_chatgpt_output(query, str(context))

    logging.info("Moving on with chatgpt output")

    test_case = LLMTestCase(
        input=str(query),
        actual_output=str(result_output),
        expected_output=str(expected_output),
        retrieval_context=[str(context)],
    )
    metric = RagasMetric()
    metric.measure(test_case)
    
    context_precision = ContextualPrecisionMetric()
    context_precision.measure(test_case)
    print(context_precision.score)

    context_relevancy = ContextualRelevancyMetric()
    context_relevancy.measure(test_case)
    print(context_relevancy.score)

    context_recall = ContextRecallMetric()
    context_recall.measure(test_case)
    print(context_recall.score)

    return metric.score, result_output

qa_pairs_example = {
    #"In which city does Tiffany Morales live?": "Tiffany Morales lives in Ronaldside",
    "Where does Andrew Knox live and what is his spending history?": 
    "Andrew Knox lives in Bolivia. His spending history shows that he has made 12 purchases with a total spend of 351.49. His primary type of purchases is jeans, and his last purchase date was on October 22, 2023.",
    #"What's the name of customer who is the most dissatisfied with our products and why?": "Bradley Tanner"
    # "Who wrote 'To Kill a Mockingbird'?": "Harper Lee",
    # "What is the chemical formula for water?": ("H2O", None),
    # "Who painted the Mona Lisa?": ("Leonardo da Vinci", None),
    # "What is the speed of light?": ("299,792,458 meters per second", None),
    # "What is the largest mammal?": ("Blue Whale", None),
    # "Who discovered penicillin?": ("Alexander Fleming", None),
    # "What year did World War II end?": ("1945", None),
    # "Who is the CEO of Tesla?": ("Elon Musk", None),
    # # Note: This information is based on the status as of April 2023.
    # "What is the currency of Japan?": ("Yen", None)
}



def process_collection(dataset, collection_type):
    """
    Process a collection based on the specified collection type. If the collection type is not provided,
    it defaults to iterating through known types.

    Parameters:
    - dataset: The dataset information.
    - collection_type (optional): The type of collection to process. If not provided, a default iteration is used.

    Returns:
    Tuple of file name and file content.
    """
    # If it's naive just return the file names and their content
    if collection_type == 'naive_llm':
        file_number = dataset["dataset"]
        file_name = f"{collection_type}_dataset_{file_number}.txt"
        file_path = os.path.join("synthetic_data_3", file_name)
        try:
            with open(file_path, 'r') as file:
                file_content = file.read()
                return file_name, file_content
        except FileNotFoundError:
            raise ValueError("Unable to find the file for the 'naive_llm' collection type.")

    # If it's either structured or unstructured return context None and collection name
    # because we're gonna get the context from the qdrant client
    elif collection_type == 'unstructured':
        collection_name = collection_type + "_dataset_" + str(dataset["dataset"]) + ".txt"
    else:
        collection_name = collection_type + "_dataset_" + str(dataset["dataset"]) + "_content"
        
    return collection_name, None

def evaluate_qa_pairs(
    qa_pairs: Dict[str, Tuple[str, Optional[str]]],
    collections: list,
    specific_collection_type: str = None):


    results = []
    for collection in collections:
        for collection_type, datasets in collection.items():
            if specific_collection_type and collection_type != specific_collection_type:
                continue

            for dataset in datasets:
                file_or_collection_name, context = process_collection(dataset, collection_type)
                for query, expected_output in qa_pairs.items():
                    score, actual_output = eval_test(query, expected_output, context, file_or_collection_name)
                    result_details = {
                        "question": query,
                        "expected_answer": expected_output,
                        "actual_answer": actual_output,
                        "score": score,
                        "file_or_collection_name": file_or_collection_name,
                        "eval_type": collection_type
                    }
                    results.append(result_details)
    return results

collections = [
    #{"naive_llm": [{"dataset": 1}, {"dataset": 2}, {"dataset": 3}]},
    #{"unstructured": [{"dataset": 1}, {"dataset": 2}, {"dataset": 3}]},
    #{"structured": [{"dataset": 1}, {"dataset": 2}, {"dataset": 3}]},
    {"naive_llm": [{"dataset": 3}]},
    {"unstructured": [{"dataset": 3}]},
    #{"structured": [{"dataset": 3}]}
]

# eval_test(query="What is the capital of France?", expected_output="Paris", qdrant_collection_name="naive_llm_dataset_1.txt")

evaluate_qa_pairs(qa_pairs_example, collections)

